<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Off-policy Monte Carlo Control</title>
    <style>
        body {
            font-family: Arial, sans-serif;
        }

        .topic-title {
            font-size: 22px;
            font-weight: bold;
        }

        .topic-info {
            font-size: 16px;
            margin-top: 10px;
        }
    </style>
</head>

<body>
    <div class="topic-title">Off-policy Monte Carlo Control</div>
    <div class="topic-info">Topic Number: 3.9</div>
    <div class="topic-info">Time Allotted: 1 hours</div>
    <github-md>Okay, here's a section on "Off-Policy Monte Carlo Control" suitable for a reinforcement learning course, drawing from the provided text:

**Off-Policy Monte Carlo Control**

In on-policy methods, we learn about the policy that is currently being used to generate behavior. Off-policy methods, on the other hand, allow us to learn about a target policy, *π*, while following a different behavior policy, *µ*. This is a powerful concept, as it enables us to learn about potentially better policies without needing to exclusively explore with those policies.

**The Need for Off-Policy Methods**

Off-policy methods are particularly useful when:
*   We want to learn about multiple policies simultaneously.
*   Exploring with the optimal policy is dangerous or impossible.
*   We want to reuse experience gathered from past interactions or from other agents.

**Importance Sampling**

The core technique enabling off-policy learning is *importance sampling*.  The idea is to re-weight returns generated by the behavior policy *µ* to estimate what they would have been under the target policy *π*. We do this by using the ratio of the probabilities of taking the same actions under the two policies.  In the context of Monte Carlo, this is done using a ratio of the products of the action probabilities at each step in the episode.

*   **Importance Sampling Ratio:** For an episode, the importance sampling ratio, denoted as  *G<sup>π/µ</sup><sub>t</sub>*,  for a return at time *t* is calculated as:

    *   *G<sup>π/µ</sup><sub>t</sub>*  =  ( *π(A<sub>t</sub> | S<sub>t</sub>)* / *µ(A<sub>t</sub> | S<sub>t</sub>)* )  *  ( *π(A<sub>t+1</sub> | S<sub>t+1</sub>)* / *µ(A<sub>t+1</sub> | S<sub>t+1</sub>)* ) ... ( *π(A<sub>T</sub> | S<sub>T</sub>)* / *µ(A<sub>T</sub> | S<sub>T</sub>)* )

    where:
    *   *A<sub>t</sub>* and *S<sub>t</sub>* are the action and state at time *t*, respectively.
    *   *T* is the final time step of the episode.
    *   *π(A<sub>t</sub> | S<sub>t</sub>)* is the probability of taking action *A<sub>t</sub>* in state *S<sub>t</sub>* under the target policy *π*.
    *   *µ(A<sub>t</sub> | S<sub>t</sub>)* is the probability of taking action *A<sub>t</sub>* in state *S<sub>t</sub>* under the behavior policy *µ*.

*   **Corrected Return:** The return *G<sub>t</sub>* observed under *µ* is then scaled by this ratio to obtain an estimate of the return under *π*:  *G<sup>π/µ</sup><sub>t</sub>* *G<sub>t</sub>*

*   **Value Update:** The value function is updated towards this corrected return:

    *   *V(S<sub>t</sub>) ← V(S<sub>t</sub>) + α (G<sup>π/µ</sup><sub>t</sub>  G<sub>t</sub> - V(S<sub>t</sub>))*

    where *α* is the learning rate.

**Important Caveats of Importance Sampling:**

*   **Zero Probability:** Importance sampling is not applicable if the behavior policy, *µ*, assigns zero probability to an action that the target policy, *π*, would take (i.e., if *µ(A<sub>t</sub> | S<sub>t</sub>) = 0* when *π(A<sub>t</sub> | S<sub>t</sub>) > 0*).
*   **High Variance:** Importance sampling can dramatically increase variance, especially when the policies are very different or when episodes are long. The variance arises because the importance sampling ratio is the product of many terms, each of which can be greater or less than one, leading to potentially very large or very small values.

**Weighted Importance Sampling**

To address the high variance issue, we often use *weighted importance sampling*. In this approach, the returns are averaged using a weighted sum.

*   **Weighted Average:**  Instead of a simple average of importance sampled returns, we compute a weighted average. If we have *n-1* returns *G<sub>1</sub>, G<sub>2</sub>, ..., G<sub>n-1</sub>*  with corresponding weights *W<sub>i</sub>*, the weighted average estimate is:

    *   *V<sub>n</sub> =  (∑<sup>n-1</sup><sub>k=1</sub> W<sub>k</sub> G<sub>k</sub>) / (∑<sup>n-1</sup><sub>k=1</sub> W<sub>k</sub>)*

*   **Incremental Update:**  We can update the weighted average incrementally, using a cumulative sum of weights, *C<sub>n</sub>*:

    *  *V<sub>n+1</sub> = V<sub>n</sub> + (W<sub>n</sub>/C<sub>n</sub>) [G<sub>n</sub> - V<sub>n</sub>]*
    *  *C<sub>n+1</sub> = C<sub>n</sub> + W<sub>n+1</sub>*

    where *C<sub>0</sub> = 0* and *V<sub>1</sub>* is arbitrary.   In this case, the weights *W<sub>i</sub>* are the importance sampling ratios computed for each step of the episode.

**Off-Policy Monte Carlo Control Algorithm (Every Visit, Weighted Importance Sampling)**

The following algorithm combines importance sampling with control (policy improvement):

1.  **Initialization:**
    *   For all state-action pairs *s*, *a*:
        *   Initialize action-value function *Q(s,a)* to arbitrary values.
        *   Initialize cumulative weight *C(s,a)* to 0.
    *   Initialize a deterministic policy *π(s)* that is greedy with respect to *Q*.

2.  **Repeat Forever:**
    *   Generate an episode using any *soft* behavior policy *µ*:
        *   *S<sub>0</sub>, A<sub>0</sub>, R<sub>1</sub>, ..., S<sub>T-1</sub>, A<sub>T-1</sub>, R<sub>T</sub>, S<sub>T</sub>*
    *   Initialize the return *G* to 0 and the importance weight *W* to 1.
    *   **For** *t* = *T-1, T-2, ..., 0* **do**:
        *   *G ← γG + R<sub>t+1</sub>*
        *   *C(S<sub>t</sub>, A<sub>t</sub>) ← C(S<sub>t</sub>, A<sub>t</sub>) + W*
        *   *Q(S<sub>t</sub>, A<sub>t</sub>) ← Q(S<sub>t</sub>, A<sub>t</sub>) + (W/C(S<sub>t</sub>, A<sub>t</sub>)) [G - Q(S<sub>t</sub>, A<sub>t</sub>)]*
        *   *π(S<sub>t</sub>) ← argmax<sub>a</sub> Q(S<sub>t</sub>, a)*  (break ties arbitrarily)
        *   *W ← W  (π(A<sub>t</sub> | S<sub>t</sub>) / µ(A<sub>t</sub> | S<sub>t</sub>))*
        *   **If** *W = 0* **then** Exit For Loop

**Explanation:**

*   The algorithm initializes the action-value function *Q* and the cumulative weights *C*.
*   It generates episodes following a soft behavior policy *µ*. A soft policy is one that gives non-zero probability to all actions (to ensure we can explore all actions).
*   The loop goes backwards through the episode, calculating the return and updating the *Q* value.
*   The policy *π* is updated to always select the action with the highest Q value.
*   The importance sampling ratio is applied to the weight, *W*.
*   If the weight *W* becomes zero, it means that an action under the behavior policy had 0 probability when it would have had a non-zero probability under the target policy, which is a case where the importance sampling formula is undefined. The loop is exited in this case.
*   This algorithm converges to the optimal policy at all encountered states, even though actions are selected according to a different soft policy *µ*.

**Truncated Returns**

The provided text also touches on the concept of *truncated returns* as a way to reduce variance, especially when the discount factor *γ* is less than 1. Instead of multiplying the importance sampling ratio over the entire episode, we can apply the importance sampling ratio only to the beginning of the return, because the rest of the return is fixed after the first step, given that the reward is independent of any future actions. This is particularly useful when episodes are long and *γ* is significantly less than 1.

**Key Differences from On-Policy Monte Carlo:**

*   On-policy methods update the policy *using* the same policy that generated the data. Off-policy methods learn about a *different* policy than the one generating the data.
*   Off-policy methods need to use importance sampling to correct for the difference between the target and behavior policies.

**Conclusion**

Off-policy Monte Carlo control is a powerful tool that allows us to learn optimal policies from data generated by different policies. This flexibility makes it applicable in a wide range of reinforcement learning scenarios. However, it's important to be aware of the challenges associated with importance sampling, such as potentially high variance.
 </github-md>
    <div class="back-link"><a href="../syllabus_structure.html">Back to Syllabus</a></div>
</body>

</html>
<script src="https://cdn.jsdelivr.net/gh/MarketingPipeline/Markdown-Tag/markdown-tag.js"></script>