<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Policy Evaluation (Prediction)</title>
    <style>
        body {
            font-family: Arial, sans-serif;
        }

        .topic-title {
            font-size: 22px;
            font-weight: bold;
        }

        .topic-info {
            font-size: 16px;
            margin-top: 10px;
        }
    </style>
</head>

<body>
    <div class="topic-title">Policy Evaluation (Prediction)</div>
    <div class="topic-info">Topic Number: 3.1</div>
    <div class="topic-info">Time Allotted: 1 hours</div>
    <github-md>Okay, here's a course text section on "Policy Evaluation (Prediction)" based on the provided material, designed for a reinforcement learning course:

**Policy Evaluation (Prediction)**

In reinforcement learning, policy evaluation, also sometimes referred to as prediction, is the process of determining the value function for a given policy. Recall that a policy, denoted as π, defines the learning agent's behavior, mapping perceived states of the environment to actions to be taken in those states. The goal of policy evaluation is to estimate how good a given policy is by quantifying the expected cumulative reward an agent will receive by following that policy.

**The Value Function**

The value function, denoted as vπ(s), represents the expected return when starting from state *s* and following policy *π*. The return, *Gt*, is the total discounted reward from time *t*:

*Gt* = *Rt+1* + γ*Rt+2* + ...+ γ*T-1* *RT*

Where:
*   *Rt* is the reward at time step *t*.
*   γ (gamma) is the discount factor (0 ≤ γ ≤ 1), which determines the importance of future rewards.

The value function is then defined as the expected return:

*vπ(s)* = *Eπ* [*Gt* | *St* = *s*]

**Monte Carlo Policy Evaluation**

Monte Carlo (MC) methods are a class of algorithms that learn from complete episodes of experience.  In the context of policy evaluation, Monte Carlo methods learn the value function *vπ(s)* by observing multiple episodes generated by following policy *π*. The core idea is to use the *empirical mean return* observed in these episodes instead of the expected return directly.  

Specifically, for each state *s* encountered in an episode, we record the return *Gt* following the first time that state is visited in the episode. We then estimate *vπ(s)* as the average of these observed returns for that state.

**Off-Policy Prediction via Importance Sampling**

Monte Carlo methods can also be adapted for off-policy learning, where we evaluate a *target policy* (π) using experience generated by a different *behavior policy* (µ). This is accomplished through the use of *importance sampling*.

There are two forms of importance sampling to consider:

1.  **Ordinary Importance Sampling**: The value function estimate is given by:

    *V(s)* =  ∑  *ρT(t)* *Gt*  /  *N*

   where:
    *  *N* is the number of times state *s* is visited in the generated episodes.
    *  *ρT(t)* is the importance sampling ratio which represents the likelihood of observing the trajectory under the target policy compared to the behavior policy.
    *  *Gt* is the return observed in episode *t*.

    This method is unbiased, meaning that its expected value converges to the true *vπ(s)*. However, the variance can be extremely high, even unbounded, making it unreliable in practice due to the unbounded nature of the ratios.

2. **Weighted Importance Sampling**: This method uses a weighted average of returns:

    *V(s)* = ∑ *ρT(t)* *Gt* / ∑ *ρT(t)*

    or zero if the denominator is zero.

    In this approach, each return is weighted by its corresponding importance sampling ratio.  A key property of the weighted estimate is that when observing a single return, the ratio for that return cancels out, making the estimate equal to the observed return. Although this method is biased, the variance of this estimator is generally much lower than that of ordinary importance sampling. The weighted estimate has proven to be much more robust in practice and is therefore strongly preferred.

**Summary**

Policy evaluation is a fundamental step in reinforcement learning, allowing us to estimate the value of a given policy. Monte Carlo methods provide a way to estimate these values from observed experience, with off-policy importance sampling extending the application of these methods. Weighted importance sampling is generally preferred in practice due to its lower variance.

**Explanation of Changes and Additions**

*   **Structure:** The text is organized into logical sections: Introduction, Value Function, Monte Carlo, and Off-Policy with Importance Sampling. This provides a clear flow for learning.
*   **Definitions and Notation:** Key terms like "policy," "value function," "return," "discount factor," and *vπ(s)* are defined clearly. The mathematical notation is explained.
*   **Monte Carlo Details:** The explanation of Monte Carlo policy evaluation is made clearer by noting that the empirical return is used to estimate value function.
*   **Off-Policy Importance Sampling:** The two importance sampling methods are discussed in detail, including their formulas. The text emphasizes the pros and cons of each method, especially noting that weighted importance sampling is preferred due to lower variance.
*   **Attribution:** The text is clearly derived from the provided document.
*   **Clarity:** The language is simplified, and technical terms are defined to make the material accessible for a college course.
*   **Omitted Non-Textual Elements:** All non-textual elements, references to chapter numbers, page numbers and credits have been removed.

This revised text provides a comprehensive explanation of policy evaluation, covering all relevant aspects from the provided context and is suitable for a reinforcement learning course.
 </github-md>
    <div class="back-link"><a href="../syllabus_structure.html">Back to Syllabus</a></div>
</body>

</html>
<script src="https://cdn.jsdelivr.net/gh/MarketingPipeline/Markdown-Tag/markdown-tag.js"></script>