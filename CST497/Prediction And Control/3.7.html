<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Off-policy Prediction via Importance Sampling</title>
    <style>
        body {
            font-family: Arial, sans-serif;
        }

        .topic-title {
            font-size: 22px;
            font-weight: bold;
        }

        .topic-info {
            font-size: 16px;
            margin-top: 10px;
        }
    </style>
</head>

<body>
    <div class="topic-title">Off-policy Prediction via Importance Sampling</div>
    <div class="topic-info">Topic Number: 3.7</div>
    <div class="topic-info">Time Allotted: 1 hours</div>
    <github-md>Okay, here's course text on "Off-Policy Prediction via Importance Sampling" based on the provided lecture notes, designed for a reinforcement learning course:

**Off-Policy Prediction via Importance Sampling**

In reinforcement learning, we often want to evaluate a policy (the *target policy*, denoted as π) using data generated by a different policy (the *behavior policy*, denoted as µ). This is known as *off-policy learning*.  Directly using data from µ to evaluate π is problematic because the data distribution is not aligned with the policy we want to evaluate.  *Importance sampling* provides a way to correct for this mismatch.

**The Core Idea of Importance Sampling**

The fundamental concept behind importance sampling is to estimate the expectation of a function under one distribution by using samples from another distribution.  The general formula is:

  E<sub>X~P</sub>[f(X)] =  E<sub>X~Q</sub>[ (P(X)/Q(X)) * f(X) ]

where:
*   P is the target distribution
*   Q is the sampling distribution
* f(X) is the function we want the expectation of

In our context:

*   P is the distribution of trajectories generated by the target policy π.
*   Q is the distribution of trajectories generated by the behavior policy µ.
*  f(X) is the return of a trajectory.

**Importance Sampling for Off-Policy Monte Carlo**

When using Monte Carlo methods, we estimate the value of a state by averaging the returns observed when starting from that state. In the off-policy scenario, we use returns generated by policy µ to evaluate policy π.  To do this, we weight each return (G<sub>t</sub>) by the ratio of the probabilities of the actions taken under the two policies. This ratio, known as the *importance sampling ratio*, is calculated as:

ρ<sub>t:T</sub> =  π(A<sub>t</sub>|S<sub>t</sub>) / µ(A<sub>t</sub>|S<sub>t</sub>) * π(A<sub>t+1</sub>|S<sub>t+1</sub>) / µ(A<sub>t+1</sub>|S<sub>t+1</sub>) * ... * π(A<sub>T</sub>|S<sub>T</sub>) / µ(A<sub>T</sub>|S<sub>T</sub>)

Where:
*   t is the start time of the return
*   T is the end time of the episode
* A<sub>i</sub> is the action taken at time i
* S<sub>i</sub> is the state at time i

This ratio is multiplied by the observed return (G<sub>t</sub>) to get the *corrected return* (G<sup>π/µ</sup><sub>t</sub>):

G<sup>π/µ</sup><sub>t</sub> = ρ<sub>t:T</sub> * G<sub>t</sub>

The value function can then be updated using this corrected return:

V(S<sub>t</sub>)  <- V(S<sub>t</sub>) + α (G<sup>π/µ</sup><sub>t</sub> - V(S<sub>t</sub>) )

**Important Consideration**: If the behavior policy µ assigns zero probability to an action that the target policy π would take, this ratio becomes undefined. Therefore, importance sampling cannot be used in such cases.

**Simple vs. Weighted Importance Sampling**

There are two main ways to use importance sampling:

1.  **Simple (Ordinary) Importance Sampling:** This method uses a simple average of the corrected returns.  While this estimator is *unbiased* (meaning its expectation equals the true value), it can have a very high variance.  A single trajectory that is much more likely under the target policy can drastically inflate or deflate the estimate.

2.  **Weighted Importance Sampling:** This method uses a weighted average, defined as:

   V(s) =  ( ∑<sub>t∈T(s)</sub> ρ<sub>t:T</sub> * G<sub>t</sub> ) /  ( ∑<sub>t∈T(s)</sub> ρ<sub>t:T</sub>)

    or zero if the denominator is zero.
    Where T(s) is the set of all time steps where state s is visited

The key difference is that in the weighted average, the importance ratio for a single return cancels out in the numerator and denominator, so the estimate is equal to the observed return. Although this estimator is *biased*, it typically has much lower variance and is strongly preferred in practice. In fact, the variance of the weighted importance-sampling estimator converges to zero, even if the variance of the ratios themselves is infinite, assuming bounded returns.

**Importance Sampling for Off-Policy TD Learning**

Importance sampling can also be applied to Temporal Difference (TD) learning.  In this case, the TD target is weighted by an importance sampling ratio.  However, unlike Monte Carlo, we only need *one* importance sampling correction for the current step:

V(S<sub>t</sub>)  <- V(S<sub>t</sub>) + α ( (π(A<sub>t</sub>|S<sub>t</sub>) / µ(A<sub>t</sub>|S<sub>t</sub>)) * (R<sub>t+1</sub> + γV(S<sub>t+1</sub>)) - V(S<sub>t</sub>) )

This approach significantly reduces variance compared to Monte Carlo importance sampling, and also relaxes the constraint of policy similarity to a single step rather than over the entire episode.

**Summary**

Importance sampling is a powerful tool for off-policy learning.  It allows us to evaluate a target policy using data generated by a different behavior policy. However, it's critical to be aware of the potential for high variance with simple importance sampling and to consider using weighted importance sampling instead. When applied to TD methods, importance sampling is even more efficient.
 </github-md>
    <div class="back-link"><a href="../syllabus_structure.html">Back to Syllabus</a></div>
</body>

</html>
<script src="https://cdn.jsdelivr.net/gh/MarketingPipeline/Markdown-Tag/markdown-tag.js"></script>