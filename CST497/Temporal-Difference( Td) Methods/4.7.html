<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>n-step Off-policy Learning</title>
    <style>
        body {
            font-family: Arial, sans-serif;
        }

        .topic-title {
            font-size: 22px;
            font-weight: bold;
        }

        .topic-info {
            font-size: 16px;
            margin-top: 10px;
        }
    </style>
</head>

<body>
    <div class="topic-title">n-step Off-policy Learning</div>
    <div class="topic-info">Topic Number: 4.7</div>
    <div class="topic-info">Time Allotted: 1 hours</div>
    <github-md>Okay, here's a course text section on "n-step Off-policy Learning" synthesized from the provided lecture notes, along with explanations:

**Course Text: n-step Off-Policy Learning**

**Introduction to Off-Policy Learning**

In reinforcement learning, we often want to learn about a target policy, denoted as π, using data generated from a different behavior policy, denoted as µ. This is known as **off-policy learning**. This contrasts with on-policy learning, where we learn about a policy from data generated by that same policy. Off-policy methods are useful when:

*   We want to learn from data collected by an older policy or by a human demonstrator.
*   We want to explore more broadly (using µ) while still learning about the optimal policy (π).
*   We want to learn about multiple policies simultaneously.

As the lecture notes say, "Oﬀ-policy learning" is like "looking over someone's shoulder", where we learn about a target policy π from experience sampled from a different behavior policy µ. In contrast, "on-policy learning" is like "learning on the job," where we learn about policy π from experience sampled from π itself.

**Off-Policy Learning with Importance Sampling**

A key technique for off-policy learning is **importance sampling**. The basic idea is to correct for the difference between the behavior policy µ and the target policy π.

1.  **Off-Policy Monte Carlo with Importance Sampling**

    *   When using Monte Carlo methods, which learn from complete episodes, we want to use returns generated from µ to evaluate π.
    *   To do this, we weight each return *G<sub>t</sub>* according to the similarity between the policies π and µ. This weighting is done by multiplying by a ratio of the probabilities of the actions taken under the target and behavior policies for the entire episode:
         
        *G<sub>t</sub><sup>π/µ</sup>* =  π(A<sub>t</sub> | S<sub>t</sub>) / µ(A<sub>t</sub> | S<sub>t</sub>)  *  π(A<sub>t+1</sub> | S<sub>t+1</sub>) / µ(A<sub>t+1</sub> | S<sub>t+1</sub>) ... π(A<sub>T</sub> | S<sub>T</sub>) / µ(A<sub>T</sub> | S<sub>T</sub>) * G<sub>t</sub>* 
    
        *   Where *G<sub>t</sub>* is the return from time t to the end of the episode, and *T* is the final time step.
        *   This ratio is called the importance sampling ratio. The product of the ratios for each step in the episode is the *importance sampling correction* for the full episode.
    *   The value function *V(S<sub>t</sub>)* is then updated toward the corrected return:

        *   *V(S<sub>t</sub>) ← V(S<sub>t</sub>) + α (G<sub>t</sub><sup>π/µ</sup> - V(S<sub>t</sub>))*

    *   **Important Note**: This method cannot be used if the behavior policy µ assigns zero probability to an action that the target policy π would take (i.e. if µ(A<sub>t</sub>|S<sub>t</sub>) = 0 and π(A<sub>t</sub>|S<sub>t</sub>) > 0). In this case, the ratio would be undefined. Also, importance sampling can dramatically increase the variance of the updates, making learning unstable.

2. **Off-Policy TD with Importance Sampling**

    *   Instead of waiting until the end of the episode, we can use Temporal Difference (TD) learning, which updates the value function after each step.
    *   We use TD targets generated from µ to evaluate π. We weight the TD target *R<sub>t+1</sub> + γV(S<sub>t+1</sub>)* by a single importance sampling ratio.
    *   The update rule becomes:
    
        *  *V(S<sub>t</sub>) ← V(S<sub>t</sub>) + α (π(A<sub>t</sub> | S<sub>t</sub>) / µ(A<sub>t</sub> | S<sub>t</sub>)  (R<sub>t+1</sub> + γV(S<sub>t+1</sub>)) - V(S<sub>t</sub>))*
        
    *   This method has much lower variance than Monte Carlo importance sampling.
    *   The policies only need to be similar over a single step.

**Off-Policy Learning with Q-Learning**

    *   Q-learning is another off-policy method that directly learns the optimal action-value function *Q(s,a)* without using importance sampling.
    *   In Q-learning, the next action *A<sub>t+1</sub>* is chosen using the behavior policy *µ(·|S<sub>t</sub>)*. 
    *   However, we consider an *alternative successor action A'* that would be chosen under the target policy *π(·|S<sub>t</sub>)*.
    *   We then update *Q(S<sub>t</sub>, A<sub>t</sub>)* towards the value of this alternative action:
    
        *   *Q(S<sub>t</sub>, A<sub>t</sub>) ← Q(S<sub>t</sub>, A<sub>t</sub>) + α (R<sub>t+1</sub> + γ Q(S<sub>t+1</sub>, A') - Q(S<sub>t</sub>, A<sub>t</sub>))*

**n-Step Off-Policy Learning**

The provided lecture notes do not explicitly cover n-step off-policy learning. However, we can deduce how it would extend from the given information. 

*   **Combining Ideas**: We can combine the concepts of n-step returns with importance sampling. Instead of using a single-step TD target, we would use an n-step return, and apply appropriate importance sampling corrections for each step.
*   **Generalized n-step importance sampling**: The general approach would involve calculating the n-step return, and then multiplying it by the product of importance sampling ratios for the *n* steps.
*   **Trade-off**: This would give us a trade-off between the low-variance of single-step TD learning and the lower bias of Monte-Carlo.

**Explanation of how the text was created:**

1.  **Extracted relevant concepts:**
    *   The text identifies and explains the core concepts of off-policy learning, namely:
        *   The difference between on-policy and off-policy learning.
        *   The need for importance sampling.
        *   Off-policy Monte Carlo with importance sampling.
        *   Off-policy TD with importance sampling.
        *   Q-Learning as an off-policy method.
2.  **Used the text directly:** Phrases and sentences from the provided lecture notes were used verbatim whenever possible. For instance, the definitions of on-policy and off-policy learning are taken directly from the notes.
3.  **Organized the information:** The information was structured into a logical flow, starting with an introduction to off-policy learning, then moving into the two main methods with importance sampling, and finally introducing Q-learning.
4.  **Added explanations:** The text includes additional explanations to make the concepts clearer for a student. For example, the explanation of the importance sampling ratio and the conditions when it cannot be used was added for clarity.
5.  **Inferred n-step off-policy learning:** The notes did not directly talk about n-step off-policy learning. However, based on the concepts of n-step returns (which are common in RL, though not explicitly stated in these notes) and importance sampling, a reasonable explanation of what that would look like was added.
6.  **Removed non-textual elements**: Any non-textual elements like "Lecture 5" were removed as requested.

This course text should give a good overview of off-policy learning, as it is covered in the given context, along with an explanation of how to extend to the topic of n-step learning.
 </github-md>
    <div class="back-link"><a href="../syllabus_structure.html">Back to Syllabus</a></div>
</body>

</html>
<script src="https://cdn.jsdelivr.net/gh/MarketingPipeline/Markdown-Tag/markdown-tag.js"></script>