<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Q-learning: Off-policy TD Control</title>
    <style>
        body {
            font-family: Arial, sans-serif;
        }

        .topic-title {
            font-size: 22px;
            font-weight: bold;
        }

        .topic-info {
            font-size: 16px;
            margin-top: 10px;
        }
    </style>
</head>

<body>
    <div class="topic-title">Q-learning: Off-policy TD Control</div>
    <div class="topic-info">Topic Number: 4.4</div>
    <div class="topic-info">Time Allotted: 1 hours</div>
    <github-md>Okay, here's a course text on "Q-learning: Off-policy TD Control", drawing from the provided lecture notes, with explanations and attribution:

**Q-learning: Off-Policy TD Control**

In reinforcement learning, we often want to learn an optimal policy without being constrained by the actions we're currently taking. This is where off-policy learning comes into play. Q-learning is a powerful off-policy Temporal Difference (TD) control algorithm that allows us to do just that.

**Core Idea**

The fundamental idea behind Q-learning is that we learn an action-value function, *Q(s, a)*, that estimates the expected cumulative reward of taking action *a* in state *s* and then following some policy thereafter. Critically, in Q-learning, the policy we use to *select* actions (the behavior policy) and the policy we are trying to *evaluate and improve* (the target policy) can be different. This contrasts with on-policy methods where the behavior and target policy are the same.

**Off-Policy Nature**

*   **Behavior Policy (µ):** We use a behavior policy, often denoted as µ, to choose actions in the environment. This policy might be exploratory, like an ε-greedy policy, which balances exploration (trying new actions) and exploitation (choosing actions that have been good in the past).
*   **Target Policy (π):** The target policy, denoted as π, is the policy we are actually trying to learn and optimize. In Q-learning, the target policy is typically a greedy policy with respect to the current estimate of the Q-function. That is, we choose the action that maximizes the Q-value.

**The Q-Learning Update**

The core of Q-learning is its update rule, which is a TD update:

   *Q(St, At) ← Q(St, At) + α * [Rt+1 + γ * max<sub>a'</sub> Q(St+1, a') - Q(St, At)]*

Let's break this down:

*   **St, At:** The current state and the action taken in that state.
*   **Rt+1:** The reward received after taking action *At* in state *St*.
*   **St+1:** The next state reached after taking action *At* in state *St*.
*   **α:** The learning rate, determines the step size.
*   **γ:** The discount factor, which determines how much future rewards are valued.
*   **max<sub>a'</sub> Q(St+1, a'):** This is the key part that makes Q-learning off-policy. Instead of using the action that we actually took from *St+1* according to our behavior policy, we use the *maximum* Q-value for any possible action in state *St+1*. This represents our target policy.

**Explanation of the Update**

1.  We start with the current Q-value for the state-action pair (St, At).
2.  We observe the reward *Rt+1* and transition to state *St+1*.
3.  Instead of using the *actual action* in *St+1* according to our behavior policy µ, we consider *all* possible actions in *St+1* and pick the one that maximizes *Q(St+1,a')*. This reflects our target policy *π*, which is greedy with respect to Q.
4.  We calculate the temporal difference error (TD error), which is the difference between the estimated return and the current Q-value.
5.  We update our Q-value for (St, At) by moving it in the direction of the TD error, scaled by the learning rate *α*.

**Why No Importance Sampling?**

Unlike some off-policy methods like off-policy TD with importance sampling, Q-learning does not require importance sampling. We don't need to explicitly correct for the difference between the behavior policy and the target policy. This is because the Q-learning update directly targets the optimal Q-function, regardless of how we explore the state space.

**Advantages of Q-Learning**

*   **Off-policy:** The ability to learn from behavior generated by a different policy is a powerful advantage. It allows for learning from past experiences or from data collected using a different exploration strategy.
*   **Simplicity:** The update rule is relatively straightforward to implement.
*   **Convergence (under certain conditions):** Q-learning is guaranteed to converge to the optimal Q-function, if you use an appropriate exploration strategy (like epsilon greedy) and a few other conditions are met.

**Summary**

Q-learning is an off-policy TD control algorithm that learns an optimal action-value function by updating estimates towards the maximum Q-value of the next state, rather than the value of the action that was actually taken. This allows us to learn an optimal policy even when actions are selected using an exploratory behavior policy. It is a fundamental algorithm in reinforcement learning and is widely used in practice.
 </github-md>
    <div class="back-link"><a href="../syllabus_structure.html">Back to Syllabus</a></div>
</body>

</html>
<script src="https://cdn.jsdelivr.net/gh/MarketingPipeline/Markdown-Tag/markdown-tag.js"></script>